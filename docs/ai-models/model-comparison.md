# Сравнение открытых моделей для проверки лабораторных/курсовых

Ниже — ориентировочная сравнительная таблица открытых моделей и стеков, которые можно применить для автоматизированной проверки студенческих работ (код, отчёты, пояснения). Оценки условные и должны быть уточнены пилотами.

| Модель / Стек | Тип задач | Лицензия | Ресурсы (минимум) | Плюсы | Минусы | Примечания |
|---|---|---|---|---|---|---|
| Code Llama 7B Instruct | Пояснения к коду, простая рубрика | Meta Llama 2 CC BY-NC | 1×GPU 16GB или vLLM CPU (медленно) | Хорош для кода, инструктаж | Неготов для сложной аргументации, лицензия non-commercial | Для академ. некоммерч. ок; проверьте политику вуза |
| Llama 3.1 8B Instruct (community weights) | Общее объяснение/суммаризация | Meta Llama 3 custom (условия) | 1×GPU 16-24GB | Сильный general-purpose | Может галлюцинировать | Нужны строгие схемы output |
| Mistral 7B Instruct | Объяснения, краткие комментарии | Apache 2.0 | 1×GPU 16GB | Либеральная лицензия | Слабее на сложных задачах | Хороший baseline |
| Qwen2.5 7B Instruct | Объяснение, классификация | Apache 2.0 | 1×GPU 16-24GB | Сильный на reasoning среди 7B | Требует тонкой настройки | Поддержка в vLLM хорошая |
| Phi-3 Mini (3.8B) | Короткие комментарии, быстрый отклик | MIT | CPU возможен, GPU 8-12GB лучше | Очень дешёвый/быстрый | Ограниченная глубина | Как rule-assist / sanity-check |
| StarCoder2 7B | Разбор кода, completion | BigCode OpenRAIL | 1×GPU 16-24GB | Хорош на коде | Не instr-tuned | Нужна донастройка |
| Whisper (OpenAI) / Whisper.cpp | ASR для устных защит | MIT | CPU ок, лучше VAD | Надёжен | Не про текст/код | Вспомогательная модель |
| LaMini-LM / Distil-LLM | Лёгкие LLM | Разные | CPU/GPU 8-16GB | Лёгкость | Меньше качество | Как fallback |
| Text Embeddings: bge-base-en/bge-m3 | Классификация критериев, поиск похожих | Apache 2.0 | CPU/GPU | Сильные эмбеддинги | Нужен свой классификатор | Для similarity/классификаторов |

Примечание: для задач точного скоринга по рубрикам рекомендуем ансамбль: эмбеддинги + эвристики + LLM для пояснений + строгая валидация JSON.

## Рекомендованные конфигурации для PoC

- Вариант A (минимум GPU):
  - vLLM + Mistral 7B Instruct (или Qwen2.5 7B) для объяснений,
  - bge-m3 для эмбеддингов (классификаторы критериев),
  - простые эвристики/линтеры для объективных проверок.
- Вариант B (CPU-first):
  - Phi-3 Mini для быстрых комментариев,
  - bge-base-en на CPU,
  - жёсткая фильтрация и короткие контексты.

## Инференс стеки (open source)

- vLLM — высокопроизводительный сервер LLM, легко заливается в KServe (custom runtime)
- Ollama — локальный запуск моделей, обёртка HTTP; можно завернуть в KServe контейнер
- Triton Inference Server — для ONNX/TensorRT/ensemble (подходит для эмбеддингов/классификаторов)

